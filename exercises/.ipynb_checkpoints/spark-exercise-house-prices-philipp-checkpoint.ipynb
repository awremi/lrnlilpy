{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Open Team Exercise: Predicting House Prices\n",
    "\n",
    "![](graphics/house-for-sale-sign.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, we are going to build another predictive model using machine learning. Our goal is to predict real estate prices, given various attributes of the building.  The main difference to our previous example is that the target variable we are interested in, the sale price, is now a continuous range of values rather than a discrete set of classes. Time to recall the concepts of **classification** and **regression**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification vs Regression\n",
    "\n",
    "We speak of **classification** if the model outputs a _categorical_ variable, i.e. assigns labels to data points that divide them into groups. The machine learning algorithm often performs this task by creating and optimizing a **decision boundary** in the feature space that separates classes. (The previous chapter introduced an example of a predictive classification model.)\n",
    "\n",
    "We speak of **regression** if the target variable is a _continuous_ value. This is the task of [ðŸ““fitting](../stats/stats-fitting-short.ipynb) a function to the data points so that it enables prediction.\n",
    "\n",
    "![](https://upload.wikimedia.org/wikipedia/commons/1/13/Main-qimg-48d5bd214e53d440fa32fc9e5300c894.png)\n",
    "**classification**\n",
    "_Source: [Wikimedia Commons](https://commons.wikimedia.org/wiki/File:Main-qimg-48d5bd214e53d440fa32fc9e5300c894.png)_\n",
    "\n",
    "![](https://upload.wikimedia.org/wikipedia/commons/thumb/3/3a/Linear_regression.svg/500px-Linear_regression.svg.png) **regression** _Source: [Wikimedia Commons](https://commons.wikimedia.org/wiki/File:Linear_regression.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"../.assets/data/house/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls {data_dir}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The documentation of the data set contains explanation for the numerous attributes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat {data_dir}/data_description.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A quick look into the data file reveals a typical CSV file - we are going to parse it into a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!head {data_dir}/prices.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "After creating a `SparkSession`, we read the contents of the .csv file into a DataFrame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = pyspark.sql.SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"HousePricePredictor\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = spark.read \\\n",
    "    .format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .load(f\"{data_dir}/prices.csv\") \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining a schema for this large dataframe beforehand is a daunting task, so we leave the types a the default (string) and cast later as needed. We know however that the prices should be floating point numbers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.withColumn(\"SalePrice\", data[\"SalePrice\"].cast(\"DOUBLE\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This DataFrame has a large number of columns - let's select some to take a brief look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[[\"OverallQual\", \"OverallCond\", \"YearBuilt\", \"SalePrice\"]].show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your task now is to build a predictive model for house prices, using `prices.csv` as training data.\n",
    "\n",
    "- Build your pipeline using the building blocks provided by `pyspark.ml` (Estimator, Transformer, Pipeline...). Go back to our [ðŸ““previous classification pipeline](../spark/spark-ml-pipeline.ipynb) for inspiration.\n",
    "- `pyspark.ml` provides [**a few algorithms for regression**](https://spark.apache.org/docs/latest/ml-classification-regression.html#regression) - use both reasoning and experimentation to select a viable one.\n",
    "- Don't overcomplicate things at first - start by building a **minimal viable model** that uses a few strong features, and evaluate it - then add more features to improve performance.\n",
    "- The performance of your predictive model is going to be evaluated in the section below. Take a look at the evaluation code and the error metrics used. Make sure to use the following naming conventions so the code below gets the right inputs:\n",
    "    - `pipeline`: `pyspark.ml.Pipeline` object representing the entire ML pipeline that produces your model \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workspace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write your ML pipeline code here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Transformer, Estimator, Pipeline, PipelineModel\n",
    "from pyspark.ml.feature import OneHotEncoderEstimator, StringIndexer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initital feature selection\n",
    "First I take a look at all the columns and decide, which one I want to use at first. I choose numeric attributes like number of square feet of different parts of the house or the number of bedrooms, but also \"string\" attributes like the quality description of different parts of the house. In this case, the latter are quite easy to translate to numeric attributes the machine will understand (f.e. for Basement Quality \"Ex\" (Excellent) is a 1, \"Po\" (Poor) is a 6). I even choose to take one categorical attribute into account, the \"SaleCondition\". Because there are only a handful of options for this attribute, I will choose One-Hot-Encoding to prepare it for machine learning. I also split the features into numeric features, features which are some kind of rating and easily convertible to numbers, and the categorical feature. This way it should be easy to add more features later on if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_features = ['OverallQual', 'OverallCond', 'YearBuilt', 'BsmtFinSF1', 'BsmtUnfSF', 'TotalBsmtSF', \n",
    "                    '1stFlrSF', '2ndFlrSF', 'LowQualFinSF', 'GrLivArea', 'BsmtFullBath', 'BsmtHalfBath',\n",
    "                    'FullBath', 'HalfBath', 'TotRmsAbvGrd', 'Fireplaces', 'GarageCars', 'GarageArea', \n",
    "                    'WoodDeckSF', 'OpenPorchSF', 'EnclosedPorch', '3SsnPorch', 'ScreenPorch',\n",
    "                    'PoolArea']\n",
    "rating_features = ['ExterQual', 'ExterCond', 'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1',\n",
    "                   'HeatingQC', 'CentralAir', 'KitchenQual', 'Functional', 'FireplaceQu', 'GarageFinish',\n",
    "                   'GarageQual', 'GarageCond', 'PavedDrive', 'PoolQC', 'Fence']\n",
    "categorical_features = ['SaleCondition']\n",
    "\n",
    "features = numeric_features+rating_features+categorical_features\n",
    "\n",
    "target = ['SalePrice']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, we cannot simply use show to display the features in an easy way. We have to split the visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[features].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[features[:10]].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[features[10:20]].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[features[20:30]].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[features[30:40]].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[features[40:]].show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So let's build a simple transformer which shortens our data to only the features we specified here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColumnDropper(Transformer):\n",
    "    \"\"\"\n",
    "    Reduce data to only a subset of columns\n",
    "    \"\"\"\n",
    "    \n",
    "    # lazy workaround - a transformer needs to have these attributes\n",
    "    # TODO: replace if needed\n",
    "    _defaultParamMap = dict()\n",
    "    _paramMap = dict()\n",
    "    _params = dict()\n",
    "    uid = 0\n",
    "\n",
    "    def __init__(self, cols=None):\n",
    "        self.cols = cols\n",
    "\n",
    "\n",
    "    def _transform(self, data):\n",
    "        dataAfterDrop = data[self.cols] \n",
    "        return dataAfterDrop\n",
    "    \n",
    "    def __repr__(self):\n",
    "        \"\"\" Show a proper string representation when printing the pipeline stage\"\"\"\n",
    "        return str(type(self))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "select_pipeline = Pipeline(stages=[ColumnDropper(cols=features+target)])\n",
    "data_short = select_pipeline.fit(data).transform(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking for NaNs\n",
    "Let's use the code from the previous example to check for missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for col in data_short.columns:\n",
    "    print(col, \" : \", data_short.filter(f\"{col} is NULL\").count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No missing values!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting the attribute data types\n",
    "\n",
    "Now I can start converting the data types of the attributes. For numeric features this is simple, it gets hard however when I try convert a rating-type attribute to numeric values. Unfortunately, the data does not follow a uniform convention on how to rate things. For every type of rating, I have to build a translation table... This could be automated using the documentation file, but in this case (because I do not expect to do this again) I choose the manual approach. I create a table index to specify which column uses which table, and then I create the translation tables itself, where every string value is given a numeric value. *FÃ¼r Christian: SchÃ¶nes Beispiel dafÃ¼r, dass man sich viel Arbeit und Geld sparen kann, wenn man sich schon bei der Datenerstellung/Sammlung Gedanken darÃ¼ber macht wie sie spÃ¤ter benutzt werden kÃ¶nnten!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_index = {'ExterQual' : 0,\n",
    "               'ExterCond' : 0,\n",
    "               'BsmtQual' : 0,\n",
    "               'BsmtCond' : 0,\n",
    "               'BsmtExposure' : 1,\n",
    "               'BsmtFinType1' : 2,\n",
    "               'HeatingQC' : 0,\n",
    "               'CentralAir' : 3,\n",
    "               'KitchenQual' : 0,\n",
    "               'Functional' : 4,\n",
    "               'FireplaceQu' : 0,\n",
    "               'GarageFinish' : 5,\n",
    "               'GarageQual' : 0,\n",
    "               'GarageCond' : 0,\n",
    "               'PavedDrive' : 6,\n",
    "               'PoolQC' : 7,\n",
    "               'Fence' : 8}\n",
    "\n",
    "\n",
    "translation_tables = []\n",
    "translation_tables.append({'Ex' : '0', # I have to use strings, because df.na.replace does\n",
    "                       'Gd' : '1', # not support mixed type replacing\n",
    "                       'TA' : '2',\n",
    "                       'Fa' : '3',\n",
    "                       'Po' : '4',\n",
    "                       'NA' : '5'})\n",
    "\n",
    "translation_tables.append({'Gd' : '0',\n",
    "                       'Av' : '1',\n",
    "                       'Mn' : '2',\n",
    "                       'No' : '3',\n",
    "                       'NA' : '4'})\n",
    "\n",
    "translation_tables.append({'GLQ' : '0',\n",
    "                       'ALQ' : '1',\n",
    "                       'BLQ' : '2',\n",
    "                       'Rec' : '3',\n",
    "                       'LwQ' : '4',\n",
    "                       'Unf' : '5',\n",
    "                       'NA' : '6'})\n",
    "\n",
    "translation_tables.append({'N' : '0',\n",
    "                       'Y' : '1'})\n",
    "\n",
    "translation_tables.append({'Typ' : '0',\n",
    "                       'Min1' : '1',\n",
    "                       'Min2' : '2',\n",
    "                       'Mod' : '3',\n",
    "                       'Maj1' : '4',\n",
    "                       'Maj2' : '5',\n",
    "                       'Sev' : '6',\n",
    "                       'Sal' : '7'})\n",
    "\n",
    "translation_tables.append({'Fin' : '0',\n",
    "                       'RFn' : '1',\n",
    "                       'Unf' : '2',\n",
    "                       'NA' : '3'})\n",
    "\n",
    "translation_tables.append({'Y' : '0',\n",
    "                       'P' : '1',\n",
    "                       'N' : '2'})\n",
    "\n",
    "translation_tables.append({'Ex' : '0',\n",
    "                       'Gd' : '1',\n",
    "                       'TA' : '2',\n",
    "                       'Fa' : '3',\n",
    "                       'NA' : '4'})\n",
    "\n",
    "translation_tables.append({'GdPrv' : '0',\n",
    "                       'MnPrv' : '1',\n",
    "                       'GdWo' : '2',\n",
    "                       'MnWw' : '3',\n",
    "                       'NA' : '4'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class TypeConverter(Transformer):\n",
    "    \"\"\"\n",
    "    Converts set of columns to numeric types\n",
    "    \"\"\"\n",
    "    \n",
    "    # lazy workaround - a transformer needs to have these attributes\n",
    "    # TODO: replace if needed\n",
    "    _defaultParamMap = dict()\n",
    "    _paramMap = dict()\n",
    "    _params = dict()\n",
    "    uid = 0\n",
    "\n",
    "    def __init__(self, numeric_features=None, rating_features=None, table_index=None, translation_tables=None):\n",
    "        self.numeric_features = numeric_features # List of all numeric columns\n",
    "        self.rating_features = rating_features # List of all rating columns\n",
    "        self.table_index = table_index # What translation table to use for what rating column\n",
    "        self.translation_tables = translation_tables\n",
    "\n",
    "\n",
    "    def _transform(self, data):\n",
    "        for col in data.columns:\n",
    "            if col in self.numeric_features:\n",
    "                data = (data.withColumn(col, data[col].cast(\"double\")))\n",
    "            elif col in self.rating_features:\n",
    "                trans_table_nr = self.table_index[col]\n",
    "                data = data.na.replace(self.translation_tables[trans_table_nr], value=None, subset=col)\n",
    "                data = (data.withColumn(col, data[col].cast(\"double\")))\n",
    "            #else:\n",
    "                #print(\"Did not convert column \" + col + \" with transformer TypeConverter.\")\n",
    "        return data\n",
    "    \n",
    "    def __repr__(self):\n",
    "        \"\"\" Show a proper string representation when printing the pipeline stage\"\"\"\n",
    "        return str(type(self))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All we need now is to convert the attribute SaleCondition with OneHotEncoding. We also drop the temporarily created column SaleCondition_index and the column SaleCondition and proceed only with the One-Hot encoded variant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stages = []\n",
    "stages.append(StringIndexer(inputCol='SaleCondition',\n",
    "                            outputCol='SaleCondition_index')) # One-Hot Encoding does not work with strings...\n",
    "stages.append(OneHotEncoderEstimator(inputCols=['SaleCondition_index'],\n",
    "                                     outputCols=['SaleCondition_onehot']))\n",
    "stages.append(TypeConverter(numeric_features=numeric_features+target, rating_features=rating_features, \n",
    "                                                  table_index=table_index, translation_tables=translation_tables))\n",
    "features_new = features.copy()\n",
    "features_new.remove('SaleCondition')\n",
    "features_new.append('SaleCondition_onehot')\n",
    "stages.append(ColumnDropper(cols=features_new+target))\n",
    "preproc_pipeline = Pipeline(stages=stages)\n",
    "data_preproc = preproc_pipeline.fit(data_short).transform(data_short)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assembling all feature columns into one single columns and renaming the target column to label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabelRenamer(Transformer):\n",
    "    \"\"\"\n",
    "    Drops rows with at least one not-a-number element\n",
    "    \"\"\"\n",
    "    \n",
    "    # lazy workaround - a transformer needs to have these attributes\n",
    "    # TODO: replace if needed\n",
    "    _defaultParamMap = dict()\n",
    "    _paramMap = dict()\n",
    "    _params = dict()\n",
    "    uid = 0\n",
    "\n",
    "    def __init__(self, col=None):\n",
    "        self.col = col\n",
    "\n",
    "\n",
    "    def _transform(self, data):\n",
    "        dataRenamed = data.withColumnRenamed(self.col, 'label')\n",
    "        return dataRenamed\n",
    "    \n",
    "    def __repr__(self):\n",
    "        \"\"\" Show a proper string representation when printing the pipeline stage\"\"\"\n",
    "        return str(type(self))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "stages = []\n",
    "stages.append(LabelRenamer(col=target[0]))\n",
    "stages.append(VectorAssembler(inputCols=features_new, outputCol='features'))\n",
    "spark_needs_this_pipeline = Pipeline(stages=stages)\n",
    "data_sparked = spark_needs_this_pipeline.fit(data_preproc).transform(data_preproc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using linear regression to predict the target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "model_pipeline = Pipeline(stages=[RandomForestRegressor(featuresCol = 'features', labelCol = 'label')])\n",
    "data_model = model_pipeline.fit(data_sparked).transform(data_sparked)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joining all pipelines to a single pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages=[select_pipeline, preproc_pipeline, spark_needs_this_pipeline, model_pipeline])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we evaluate the performance of the regression model. A better model produces smaller errors in the predicted price. The two error metrics we use are **Root-Mean-Squared-Error (RMSE)** and **Mean Average Error (MAE)** between the predicted value and the observed sales price. In order to get robust scores with less random fluctuation, we apply **cross-validation**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_piped = pipeline.fit(data).transform(data) # Test if pipeline runs through"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse = CrossValidator(estimator=pipeline,\n",
    "                    evaluator=RegressionEvaluator(metricName=\"rmse\", labelCol=\"label\", predictionCol=\"prediction\"),\n",
    "                    estimatorParamMaps=ParamGridBuilder().build(),\n",
    "                    numFolds=4) \\\n",
    "                    .fit(data) \\\n",
    "                    .avgMetrics[0]\n",
    "\n",
    "mae = CrossValidator(estimator=pipeline,\n",
    "                    evaluator=RegressionEvaluator(metricName=\"mae\", labelCol=\"label\", predictionCol=\"prediction\"),\n",
    "                    estimatorParamMaps=ParamGridBuilder().build(),\n",
    "                    numFolds=4) \\\n",
    "                    .fit(data) \\\n",
    "                    .avgMetrics[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "team_name = \"Team 1\"\n",
    "print(\"\\t\".join([\"time\", \"team\", \"RMSE\", \"MAE\"]))\n",
    "print(\"\\t\".join([datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"), team_name, \"{0:.4f}\".format(rmse), \"{0:.4f}\".format(mae)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diagnostics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to get a better understanding of the error made by the model, plot the distribution of prices, predicted prices, and errors. This can provide useful feedback for model improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = pipeline.fit(data).transform(data)\n",
    "predicted[[\"label\", \"prediction\"]].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_pd = predicted[[\"label\", \"prediction\"]].toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn\n",
    "seaborn.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seaborn.distplot(predicted_pd[\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seaborn.distplot(predicted_pd[\"prediction\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seaborn.distplot(predicted_pd[\"label\"] - predicted_pd[\"prediction\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*TODO: distribution of error plot*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "_This notebook is licensed under a [Creative Commons Attribution 4.0 International License (CC BY 4.0)](https://creativecommons.org/licenses/by/4.0/). Copyright Â© 2018 [Point 8 GmbH](https://point-8.de)_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
