{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# About Regression\n",
    "\n",
    "This chapter introduces essential concepts for applying machine learning to **classification problems**:\n",
    "\n",
    "- **decision boundary**\n",
    "- **train-test-split**\n",
    "- **cross-validation**\n",
    "- **regressor quality metrics**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We speak of **regression** if the model outputs a _continuous_ variable, i.e. a numeric value. The machine learning algorithm often performs this task by **fitting** a function or curve to the data points so that it describes the data well. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://upload.wikimedia.org/wikipedia/commons/thumb/3/3a/Linear_regression.svg/500px-Linear_regression.svg.png)\n",
    "\n",
    "**regression** \n",
    "Source: [Wikimedia Commons](https://commons.wikimedia.org/wiki/File:Linear_regression.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML Regression Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A machine learning **regressor** outputs a statistical model that outputs a numeric value for the given data point. Just as a classifier, model is trained by supervised learning from a set of labelled examples, the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are a few examples of ML algorithms used for regression. You will recognize some of them, as many algorithms can be applied to learn regression as well as classification models:\n",
    "\n",
    "- **linear regression**\n",
    "- **decision tree**\n",
    "- **random forest**\n",
    "- **gradient boosted trees**\n",
    "- **support vector machine**\n",
    "- **artificial neural networks**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, consider **decision tree learning**. When using this ML technique for classification, the **leaves** of the learned decision tree contain prediction for classes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://upload.wikimedia.org/wikipedia/en/4/4f/GEP_decision_tree_with_numeric_and_nominal_attributes.png) \n",
    "\n",
    "_A decision tree for classification._\n",
    "\n",
    "Source: [Wikipedia](https://upload.wikimedia.org/wikipedia/en/4/4f/GEP_decision_tree_with_numeric_and_nominal_attributes.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, the leaves of the tree can just als well contain numeric values. In the 1-dimensional example below, we see that a decision tree can output a kind of step function that approximates the curve of the data points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://scikit-learn.org/stable/_images/sphx_glr_plot_tree_regression_001.png) \n",
    "\n",
    "\n",
    "\n",
    "Source: [scikit-learn documentation](https://scikit-learn.org/stable/auto_examples/tree/plot_tree_regression.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Regressors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For evaluating a regression model, most of the concepts and techniques from the chapter [ðŸ““ About Classification](../ml/ml-classification-intro.ipynb) are the same: Evaluate the model using **train-test split** or **cross-validation**, avoid overfitting, etc. However, since our regression model outputs a number from a continuous range, and not a class prediction, we have to look at other error metrics not based on counting true/fase positives/negatives.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regressor Performance/Error Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All **error metrics** (also called **loss functions**) receive as input the true values $y$ and the values $\\hat{y}$ that the model predicts. We write the number of values in each as $n$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mean Absolute Error (MAE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is one of the simplest possible metrics: The mean difference between predicted and actual values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\text{MAE}(y, \\hat{y}) = \\frac{1}{n} \\sum_{i=0}^{n-1} \\left| y_i - \\hat{y}_i \\right|$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (Root) Mean Squared Error ((R)MSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\text{MSE}(y, \\hat{y}) = \\frac{1}{n} \\sum_{i=0}^{n - 1} (y_i - \\hat{y}_i)^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MSE looks very similar to MAE, but unlike MAE it does not treat all errors equally: By squaring the difference, it penalizes large errors much more than small errors. Often, the square root of the resulting value is taken to obtain smaller numbers, yielding RMSE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### R^2 Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The $R^2$ score or **coefficient of determination** has the nice property that a perfect model has a score of 1.0, a trivial model that always outputs the **expected value** of the has a score of 0, and scores can be negative for arbitrarily bad models. It is defined as:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$R^2(y, \\hat{y}) = 1 - \\frac{\\sum_{i=0}^{n - 1} (y_i - \\hat{y}_i)^2}{\\sum_{i=0}^{n- 1} (y_i - \\bar{y})^2}$$ where $$\\bar{y} =  \\frac{1}{n} \\sum_{i=0}^{n - 1} y_i$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are just a few commonly used metrics, since the possibilities for defining an error metric are endless. Depending on the definition, the metric penalizes some errors more than others, and will steer your model into a certain direction. Always think about whether the error metric makes sense for the given regression task. Perhaps even a custom error metric, specifically chosen for the domain in which you are working, can be the right choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "_This notebook is licensed under a [Creative Commons Attribution 4.0 International License (CC BY 4.0)](https://creativecommons.org/licenses/by/4.0/). Copyright Â© 2018 [Point 8 GmbH](https://point-8.de)_\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
