{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is a test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This notebook tests whether PySpark and all packages required are installed properly and working.**\n",
    "\n",
    "Please run the cell below (select and press <kbd>SHIFT+ENTER</kbd>). You should see the following output on the last line: \n",
    "\n",
    "```python\n",
    "Congratulations, your PySpark stack is ready to go\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PySpark via Jupyter Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your Python interpreter is ready. Your version: \n",
      "        3.6.5 (default, Apr 25 2018, 14:23:58) \n",
      "[GCC 4.2.1 Compatible Apple LLVM 9.1.0 (clang-902.0.39.1)]\n",
      "        \n",
      "\n",
      "Congratulations, your PySpark stack is ready to go\n"
     ]
    }
   ],
   "source": [
    "fail = False\n",
    "try:\n",
    "    import sys\n",
    "    version_string = sys.version\n",
    "    version_parts = version_string.split(\".\")\n",
    "    major = int(version_parts[0])\n",
    "    minor = int(version_parts[1])\n",
    "    if (major) >= 3 and (minor >= 6):\n",
    "        print(f\"\"\"Your Python interpreter is ready. Your version: \n",
    "        {version_string}\n",
    "        \"\"\")\n",
    "    else:\n",
    "        print(f\"\"\"Your version of Python is older than required: \n",
    "            {version_string}\n",
    "        \"\"\")\n",
    "        fail = True\n",
    "except:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    import pandas\n",
    "except ImportError:\n",
    "    print(f\"\"\"Importing package failed: pandas\"\"\")\n",
    "    fail = True\n",
    "\n",
    "try:\n",
    "    import findspark\n",
    "    findspark.init()\n",
    "except ImportError:\n",
    "    print(f\"\"\"Importing package failed: findspark\"\"\")\n",
    "    fail = True\n",
    "    \n",
    "try:\n",
    "    import pyspark\n",
    "except ImportError():\n",
    "    print(f\"\"\"Importing package failed: pyspark\"\"\")\n",
    "    fail = True\n",
    "\n",
    "if not fail:\n",
    "    print(\"\")\n",
    "    print(f\"\"\"Congratulations, your PySpark stack is ready to go\"\"\")\n",
    "else:\n",
    "    print(\"\")\n",
    "    print(\"Your Python stack is not ready, please check error messages above\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PySpark Batch Jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now evaluate the cells below. This creates a script that is then submitted to your PySpark installation. Verify the output: You should see something like this:\n",
    "\n",
    "    ##########################################\n",
    "    PySpark uses Python version:  3.6.5 (default, Apr 25 2018, 14:23:58) \n",
    "    [GCC 4.2.1 Compatible Apple LLVM 9.1.0 (clang-902.0.39.1)]\n",
    "    Congratulations, submitting a PySpark job is working\n",
    "    ##########################################\n",
    "\n",
    "\n",
    "Make sure PySpark is using the right Python version. This can be achieved by setting the environment variable `PYSPARK_PYTHON` to the appropriate Python binary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting scripts/spark_job_test.py\n"
     ]
    }
   ],
   "source": [
    "%%file scripts/spark_job_test.py\n",
    "\n",
    "SPARK_APP_NAME='sparkjob_test'\n",
    "\n",
    "import sys\n",
    "from contextlib import contextmanager\n",
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "@contextmanager\n",
    "def use_spark_context(appName):\n",
    "    conf = SparkConf().setAppName(appName) \n",
    "    spark_context = SparkContext(conf=conf)\n",
    "\n",
    "    try:\n",
    "        print(\"starting \", SPARK_APP_NAME)\n",
    "        yield spark_context\n",
    "    finally:\n",
    "        spark_context.stop()\n",
    "        print(\"stopping \", SPARK_APP_NAME)\n",
    "\n",
    "\n",
    "with use_spark_context(appName=SPARK_APP_NAME) as sc:\n",
    "    rdd = sc.range(100)\n",
    "    print()\n",
    "    print(\"##########################################\")\n",
    "    print(\"PySpark uses Python version: \", sys.version)\n",
    "    print(\"Congratulations, submitting a PySpark job is working\")\n",
    "    print(\"##########################################\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-08-20 09:17:04 WARN  NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "2018-08-20 09:17:06 INFO  SparkContext:54 - Running Spark version 2.3.0\n",
      "2018-08-20 09:17:06 INFO  SparkContext:54 - Submitted application: sparkjob_test\n",
      "2018-08-20 09:17:06 INFO  SecurityManager:54 - Changing view acls to: cls\n",
      "2018-08-20 09:17:06 INFO  SecurityManager:54 - Changing modify acls to: cls\n",
      "2018-08-20 09:17:06 INFO  SecurityManager:54 - Changing view acls groups to: \n",
      "2018-08-20 09:17:06 INFO  SecurityManager:54 - Changing modify acls groups to: \n",
      "2018-08-20 09:17:06 INFO  SecurityManager:54 - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(cls); groups with view permissions: Set(); users  with modify permissions: Set(cls); groups with modify permissions: Set()\n",
      "2018-08-20 09:17:08 INFO  Utils:54 - Successfully started service 'sparkDriver' on port 53956.\n",
      "2018-08-20 09:17:08 INFO  SparkEnv:54 - Registering MapOutputTracker\n",
      "2018-08-20 09:17:08 INFO  SparkEnv:54 - Registering BlockManagerMaster\n",
      "2018-08-20 09:17:08 INFO  BlockManagerMasterEndpoint:54 - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "2018-08-20 09:17:08 INFO  BlockManagerMasterEndpoint:54 - BlockManagerMasterEndpoint up\n",
      "2018-08-20 09:17:08 INFO  DiskBlockManager:54 - Created local directory at /private/var/folders/kp/ws_ljfcj7nlgg3ztmd10tyrw0000gn/T/blockmgr-2c7f0a6b-42b7-491d-8168-9620907f6d9b\n",
      "2018-08-20 09:17:08 INFO  MemoryStore:54 - MemoryStore started with capacity 366.3 MB\n",
      "2018-08-20 09:17:08 INFO  SparkEnv:54 - Registering OutputCommitCoordinator\n",
      "2018-08-20 09:17:09 INFO  log:192 - Logging initialized @12901ms\n",
      "2018-08-20 09:17:09 INFO  Server:346 - jetty-9.3.z-SNAPSHOT\n",
      "2018-08-20 09:17:09 INFO  Server:414 - Started @13456ms\n",
      "2018-08-20 09:17:09 WARN  Utils:66 - Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "2018-08-20 09:17:09 WARN  Utils:66 - Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "2018-08-20 09:17:09 WARN  Utils:66 - Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n",
      "2018-08-20 09:17:09 WARN  Utils:66 - Service 'SparkUI' could not bind on port 4043. Attempting port 4044.\n",
      "2018-08-20 09:17:09 INFO  AbstractConnector:278 - Started ServerConnector@73c946f9{HTTP/1.1,[http/1.1]}{0.0.0.0:4044}\n",
      "2018-08-20 09:17:09 INFO  Utils:54 - Successfully started service 'SparkUI' on port 4044.\n",
      "2018-08-20 09:17:10 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@5e66074b{/jobs,null,AVAILABLE,@Spark}\n",
      "2018-08-20 09:17:10 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@2c815682{/jobs/json,null,AVAILABLE,@Spark}\n",
      "2018-08-20 09:17:10 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@485a8aad{/jobs/job,null,AVAILABLE,@Spark}\n",
      "2018-08-20 09:17:10 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@2d32567e{/jobs/job/json,null,AVAILABLE,@Spark}\n",
      "2018-08-20 09:17:10 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@b21301c{/stages,null,AVAILABLE,@Spark}\n",
      "2018-08-20 09:17:10 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@17b2cc37{/stages/json,null,AVAILABLE,@Spark}\n",
      "2018-08-20 09:17:10 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@548af907{/stages/stage,null,AVAILABLE,@Spark}\n",
      "2018-08-20 09:17:10 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@68451285{/stages/stage/json,null,AVAILABLE,@Spark}\n",
      "2018-08-20 09:17:10 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@6b049c4b{/stages/pool,null,AVAILABLE,@Spark}\n",
      "2018-08-20 09:17:10 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@265fa3c1{/stages/pool/json,null,AVAILABLE,@Spark}\n",
      "2018-08-20 09:17:10 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@725b6f66{/storage,null,AVAILABLE,@Spark}\n",
      "2018-08-20 09:17:10 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@32ca9a92{/storage/json,null,AVAILABLE,@Spark}\n",
      "2018-08-20 09:17:10 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@7d63ae44{/storage/rdd,null,AVAILABLE,@Spark}\n",
      "2018-08-20 09:17:10 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@26795b82{/storage/rdd/json,null,AVAILABLE,@Spark}\n",
      "2018-08-20 09:17:10 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@f703074{/environment,null,AVAILABLE,@Spark}\n",
      "2018-08-20 09:17:10 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@69883502{/environment/json,null,AVAILABLE,@Spark}\n",
      "2018-08-20 09:17:10 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@979a7f9{/executors,null,AVAILABLE,@Spark}\n",
      "2018-08-20 09:17:10 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@65b0581f{/executors/json,null,AVAILABLE,@Spark}\n",
      "2018-08-20 09:17:10 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@6b4d274d{/executors/threadDump,null,AVAILABLE,@Spark}\n",
      "2018-08-20 09:17:10 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@43453f03{/executors/threadDump/json,null,AVAILABLE,@Spark}\n",
      "2018-08-20 09:17:10 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@7470efd{/static,null,AVAILABLE,@Spark}\n",
      "2018-08-20 09:17:10 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@ad07d82{/,null,AVAILABLE,@Spark}\n",
      "2018-08-20 09:17:10 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@29d65c24{/api,null,AVAILABLE,@Spark}\n",
      "2018-08-20 09:17:10 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@7f7d1743{/jobs/job/kill,null,AVAILABLE,@Spark}\n",
      "2018-08-20 09:17:10 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@1bc5a951{/stages/stage/kill,null,AVAILABLE,@Spark}\n",
      "2018-08-20 09:17:10 INFO  SparkUI:54 - Bound SparkUI to 0.0.0.0, and started at http://192.168.2.103:4044\n",
      "2018-08-20 09:17:12 INFO  SparkContext:54 - Added file file:/Users/cls/Documents/Freelancing/Projects/point8/data-science-101/notebooks/spark/scripts/spark_job_test.py at file:/Users/cls/Documents/Freelancing/Projects/point8/data-science-101/notebooks/spark/scripts/spark_job_test.py with timestamp 1534749432570\n",
      "2018-08-20 09:17:12 INFO  Utils:54 - Copying /Users/cls/Documents/Freelancing/Projects/point8/data-science-101/notebooks/spark/scripts/spark_job_test.py to /private/var/folders/kp/ws_ljfcj7nlgg3ztmd10tyrw0000gn/T/spark-3cf816ed-b683-4dd0-aab3-a5be693b0f4e/userFiles-b8e32441-6209-46c6-ae7f-349f19946d60/spark_job_test.py\n",
      "2018-08-20 09:17:13 INFO  Executor:54 - Starting executor ID driver on host localhost\n",
      "2018-08-20 09:17:13 INFO  Utils:54 - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 53957.\n",
      "2018-08-20 09:17:13 INFO  NettyBlockTransferService:54 - Server created on 192.168.2.103:53957\n",
      "2018-08-20 09:17:13 INFO  BlockManager:54 - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "2018-08-20 09:17:13 INFO  BlockManagerMaster:54 - Registering BlockManager BlockManagerId(driver, 192.168.2.103, 53957, None)\n",
      "2018-08-20 09:17:13 INFO  BlockManagerMasterEndpoint:54 - Registering block manager 192.168.2.103:53957 with 366.3 MB RAM, BlockManagerId(driver, 192.168.2.103, 53957, None)\n",
      "2018-08-20 09:17:13 INFO  BlockManagerMaster:54 - Registered BlockManager BlockManagerId(driver, 192.168.2.103, 53957, None)\n",
      "2018-08-20 09:17:13 INFO  BlockManager:54 - Initialized BlockManager: BlockManagerId(driver, 192.168.2.103, 53957, None)\n",
      "2018-08-20 09:17:14 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@1e40087b{/metrics/json,null,AVAILABLE,@Spark}\n",
      "starting  sparkjob_test\n",
      "\n",
      "##########################################\n",
      "PySpark uses Python version:  3.6.5 (default, Apr 25 2018, 14:23:58) \n",
      "[GCC 4.2.1 Compatible Apple LLVM 9.1.0 (clang-902.0.39.1)]\n",
      "Congratulations, submitting a PySpark job is working\n",
      "##########################################\n",
      "\n",
      "2018-08-20 09:17:16 INFO  AbstractConnector:318 - Stopped Spark@73c946f9{HTTP/1.1,[http/1.1]}{0.0.0.0:4044}\n",
      "2018-08-20 09:17:16 INFO  SparkUI:54 - Stopped Spark web UI at http://192.168.2.103:4044\n",
      "2018-08-20 09:17:16 INFO  MapOutputTrackerMasterEndpoint:54 - MapOutputTrackerMasterEndpoint stopped!\n",
      "2018-08-20 09:17:16 INFO  MemoryStore:54 - MemoryStore cleared\n",
      "2018-08-20 09:17:16 INFO  BlockManager:54 - BlockManager stopped\n",
      "2018-08-20 09:17:16 INFO  BlockManagerMaster:54 - BlockManagerMaster stopped\n",
      "2018-08-20 09:17:16 INFO  OutputCommitCoordinator$OutputCommitCoordinatorEndpoint:54 - OutputCommitCoordinator stopped!\n",
      "2018-08-20 09:17:16 INFO  SparkContext:54 - Successfully stopped SparkContext\n",
      "stopping  sparkjob_test\n",
      "2018-08-20 09:17:16 INFO  ShutdownHookManager:54 - Shutdown hook called\n",
      "2018-08-20 09:17:16 INFO  ShutdownHookManager:54 - Deleting directory /private/var/folders/kp/ws_ljfcj7nlgg3ztmd10tyrw0000gn/T/spark-3cf816ed-b683-4dd0-aab3-a5be693b0f4e\n",
      "2018-08-20 09:17:16 INFO  ShutdownHookManager:54 - Deleting directory /private/var/folders/kp/ws_ljfcj7nlgg3ztmd10tyrw0000gn/T/spark-3cf816ed-b683-4dd0-aab3-a5be693b0f4e/pyspark-38a227b9-78e8-4e5f-99fe-6b85dfbb2d31\n",
      "2018-08-20 09:17:16 INFO  ShutdownHookManager:54 - Deleting directory /private/var/folders/kp/ws_ljfcj7nlgg3ztmd10tyrw0000gn/T/spark-560c2890-c6db-4b7f-975d-23bc10fa7341\n"
     ]
    }
   ],
   "source": [
    "!spark-submit scripts/spark_job_test.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "_This notebook is licensed under a [Creative Commons Attribution 4.0 International License (CC BY 4.0)](https://creativecommons.org/licenses/by/4.0/). Copyright Â© 2018 [Point 8 GmbH](https://point-8.de)_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
