{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Fundamentals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with Documentation\n",
    "\n",
    "Being able to find and work with documentation is an essential skill for any programmer. Throughout this course there will be a number of exercises. Keep in mind that not only information from the course materials, but also all of the internet is available for you for reference. \n",
    "\n",
    "For example, the PySpark API documentation can be found at **https://spark.apache.org/docs/latest/api/python**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our First Spark Program"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following we are using the third-party package `findspark` to easily locate the Spark installation on our system:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Afterwards, we are able to import the `pyspark` module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our first Spark program will be an approximation of the number $\\pi$ - not the most efficient one, but our program is going to use several fundamental features of Spark. \n",
    "\n",
    "First, we create a `SparkContext` to communicate with the Spark cluster, and give our program a name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = pyspark.SparkContext(appName=\"Pi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to the implementation of our algorithm: Here is the first function we need. It generates a random point in a square of size 1x1. (If you are wondering about the argument `x`, one argument is required for this to be used later as an argument to the `map` operation.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_point(x):\n",
    "    return (random.random(), random.random())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to define one more function: The following function is given a point $p = (x,y)$ and checks whether $x^2 + y^2$ is less than 1: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inside(p):\n",
    "    x, y = p\n",
    "    return x*x + y*y < 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code generates a large array of random points, distributes it to the cluster and calls our function once for each of them. It then counts the number of times our function returned `True` - which is approximately equal to $\\pi$. Finally, it calls the `stop` method of the Spark context to terminate the program - this is necessary before we can create a new Spark context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.14205056\n",
      "CPU times: user 20.7 ms, sys: 8.06 ms, total: 28.7 ms\n",
      "Wall time: 41.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "num_samples = 100000000\n",
    "count = sc.range(num_samples).map(random_point).filter(inside).count()\n",
    "pi = 4 * count / num_samples\n",
    "print(pi)\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Curious why this works? Read more on [how to calculate $\\pi$ via Monte Carlo approximation](https://curiosity-driven.org/pi-approximation))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SparkContext and the Architecture of a Spark Program\n",
    "\n",
    "Let us take apart the program step by step and look at each one. Our first step was to create a [`SparkContext`](https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.SparkContext)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = pyspark.SparkContext(appName=\"Pi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `SparkContext` represents the connection to a Spark cluster and allows your Spark **driver application** to access the cluster. Each driver application commands a number of **executors**. These are processes that run on the nodes of the cluster and perform the actual computation and storage operations. They remain running as long as the driver application has a `SparkContext`. A program called the **cluster manager** sits between driver and executors and manages the resources given to the Spark program - for example, fairly assigning the number of nodes given to each program when multiple users are working on the cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Architecture of a spark progarm](graphics/third-party/spark-architecture.png)\n",
    "\n",
    "*Source: [MSDN](https://msdnshared.blob.core.windows.net/media/MSDNBlogsFS/prod.evol.blogs.msdn.com/CommunityServer.Blogs.Components.WeblogFiles/00/00/01/61/78/metablogapi/3566.091415_1429_Understandi1.png)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our example program, we use the `SparkContext` to create a dataset of points in a parallel way and also distribute it to the executors. We start by creating a range of numbers up to the number of samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.23 ms, sys: 1.82 ms, total: 4.05 ms\n",
      "Wall time: 11.4 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "rdd = sc.range(num_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a call to `range`, we have created our first **Resilient Distributed Dataset** or **RDD**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resilient Distributed Datasets (RDDs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The RDD is Spark's core data type, and it is in essence a _distributed collection of data objects_. It is _resilient_ because it can cope with failure of some nodes of the cluster - and in a large distributed system, things like hardware failure become something that needs to be anticipated. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformations and Actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to generate random points, we want to map each number from our range to a new random point:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4 µs, sys: 1e+03 ns, total: 5 µs\n",
      "Wall time: 16.2 µs\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "rdd = rdd.map(random_point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next step, we want to keep only the points that pass our filter criterion defined in the  `inside` function above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 24 µs, sys: 1e+03 ns, total: 25 µs\n",
      "Wall time: 29.8 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "rdd = rdd.filter(inside)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding Lazy Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a closer look at the running time measurement: Calling the `map` and `filter` methods on the RDD containing lots of random points was finished in a fraction of a second - this is because _no actual computation on the data has happened yet_. At this point, we need to understand the concept of **lazy evaluation** and learn about the difference between Spark **transformations** and **actions**.\n",
    "\n",
    "The `filter` method is a **transformation**. It is called on an RDD and returns another RDD. What actually happens under the hood when calling the method is not the computation of an RDD. When we call a transformation, we only add an operation to an **operator graph** (to be precise, a [directed acyclic graph](https://en.wikipedia.org/wiki/Directed_acyclic_graph) of RDDs and operations). This graph is an abstract description of the computation to be performed - basically, a roadmap of how an RDD is turned into another one. When working with big data, not computing immediately (**lazy evaluation**) has several advantages. Our computation might take very long, and we can first build a sequence of operations before triggering the computation and waiting for the result. Also (without going to much into the technical details of Spark), building the operator graph first allows Spark to analyze it before and coming up with an optimized plan for the actual computation (e.g. by combining operations that can be performed together, by copying or moving around as little data as possible, ...)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![operator graph](graphics/spark-operator-graph.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us take a closer look at the transformations `map` and `filter` and illustrate what they do with the elements of the RDD:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![map-filter](graphics/spark-map-filter.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last Spark operation in our program is a call to the `count` method of the RDD, which simply counts the number of elements in it. Watch the running time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 14.4 ms, sys: 4.94 ms, total: 19.4 ms\n",
      "Wall time: 46.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "count = rdd.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`count` is not a transformation but an **action**. When we invoke an action to retrieve a result, the _actual computation is triggered_. That is, the operator graph of transforms we have built before is turned into an execution plan, and the executors perform the computation on the nodes of the cluster. Then, a result is returned to the driver process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.14175424\n"
     ]
    }
   ],
   "source": [
    "pi = 4 * count / num_samples\n",
    "print(pi)\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common Transformations and Actions\n",
    "\n",
    "Here we list some commonly used transformations and actions. [Spark provides many more](https://spark.apache.org/docs/latest/rdd-programming-guide.html).\n",
    "\n",
    "**Transformations on a single RDD** \n",
    "\n",
    "| method | purpose |\n",
    "|---|---|\n",
    "| `rdd.map(f)` | apply a function `f` to each element in the RDD and return an RDD of the result |\n",
    "| `rdd.flatMap(f)` | apply a function `f` to each element in the RDD and return an RDD of the contents of the iterators returned |\n",
    "| `rdd.filter(f)` | return an RDD consisting of only elements that pass the condition, i.e. for which function `f` returns `True` |\n",
    "| `rdd.distinct()` | return RDD with duplicates removed |\n",
    "| `rdd.sample(withReplacement, fraction, [seed])` | sample an RDD, with or without replacement |\n",
    "\n",
    "\n",
    "**Transformations on two RDDs** \n",
    "\n",
    "\n",
    "| method | purpose |\n",
    "|---|---|\n",
    "|`rdd.union(other_rdd)` | produce an RDD containing elements from both RDDs | \n",
    "| `rdd.intersection(other_rdd)` | produce an RDD containing elements in the intersection of both RDDs| \n",
    "| `rdd.subtract(other_rdd)` | produce RDD with the contents of the other RDD removed from the first one |\n",
    "\n",
    "\n",
    "\n",
    "**Actions on an RDD** \n",
    "\n",
    "| method | purpose |\n",
    "|---|---|\n",
    "|`rdd.collect()` | return all elements | \n",
    "| `rdd.count()` | return the number of elements | \n",
    "| `rdd.take(num)` | return a certain number (`num`) of elements |\n",
    "| `rdd.reduce(f)` | aggregate elements as defined in `f` in parallel and return result |\n",
    "| `rdd.reduceByKey(f)` | When called on a dataset of (key, value) pairs, returns a dataset of (key, value) pairs where the values for each key are aggregated using the given reduce function `f` |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise: Transformations and Actions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXERCISE: try the transformations and actions described above on our RDD of random points\n",
    "sc = pyspark.SparkContext(appName=\"Exercise\")\n",
    "# write your code here\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hint: Write Anonymous Functions with Lambda Expressions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have already seen how to write functions in Python with the `def` statement. Here is another way that is very frequently used in the context of Spark: Lambda expressions. Using the `lambda` keyword allows us to write down a function in a very concise way, in a single line. Lambda expressions are a key feature of the **functional programming** style."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat(a, b):\n",
    "    \"\"\"Concatenate two strings\"\"\"\n",
    "    return f\"{a}{b}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply(f, args):\n",
    "    \"\"\"Apply any function to the given arguments\"\"\"\n",
    "    print(f\"Applying {f} to {args}\")\n",
    "    return f(*args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello world'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concat(\"Hello \", \"world\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying <function concat at 0x10f9e09d8> to ('Hello ', 'world')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Hello world'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "apply(concat, (\"Hello \", \"world\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying <function <lambda> at 0x10f9e0400> to ('Hello ', 'world')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Hello world'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "apply(f=lambda a,b: f\"{a}{b}\", args=(\"Hello \", \"world\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: MapReduce\n",
    "\n",
    "You may have heard of MapReduce in the context of big data. The name MapReduce originally referred to the proprietary technology of Google, where it was pioneered, but today it refers generally to a certain **programming model** for processing big data sets with a parallel, distributed algorithm on a cluster. As the name says, it relies on two main operations, `map` and `reduce`. And as you have seen, both are available in Spark: Sparks programming model includes MapReduce as part of its much larger set of transformations and actions.  \n",
    "\n",
    "The `reduce` operation is an action that iteratively combines (=aggregates) pairs of elements, two by two, until a single element is obtained. How to combine two elements is defined by the function given as arguments, that needs to be of the form $f(x,y) \\to z$. `reduceByKey` is a similar action, but works only on RDDs containing key-value-pairs, and aggregates the values with the same key.\n",
    "\n",
    "![reduce](graphics/spark-reduce.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an exercise, try impelemting our $\\pi$ approximation algorithm with `map` and `reduce` instead of `filter` and `count`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.14215472\n"
     ]
    }
   ],
   "source": [
    "# once again, the original implementation with filter and count\n",
    "sc = pyspark.SparkContext(appName=\"Pi\")\n",
    "count = sc.range(num_samples).map(random_point).filter(inside).count()\n",
    "pi = 4 * count / num_samples\n",
    "print(pi)\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXERCISE: approximation of pi using only range, map and reduce\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intermediate RDDs in the operator graph of our program are computed on demand when asking for a result via an action. In our example program, the filtered version of the RDD (containing only the points for which the `inside` function has returned `True`) is recomputed every time we call `count`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = pyspark.SparkContext(appName=\"PiCached\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12.3 ms, sys: 4.98 ms, total: 17.3 ms\n",
      "Wall time: 43.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "count = sc.range(num_samples).map(random_point).filter(inside).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 13.4 ms, sys: 5.89 ms, total: 19.3 ms\n",
      "Wall time: 40.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "count = sc.range(num_samples).map(random_point).filter(inside).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is often efficient when working with big data. However, in some cases (think of an iterative algorithm that needs to go over the intermediate RDD many times), it is more efficient to store an intermediate RDD rather than recomputing it. This can be done via Spark's functionality for **caching**. Here, we try to cache the RDD containing the points, then filtering and counting twice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "cached_rdd = sc.range(num_samples).map(random_point).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 13 ms, sys: 3.46 ms, total: 16.5 ms\n",
      "Wall time: 1min 10s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "count = cached_rdd.filter(inside).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12 ms, sys: 5.31 ms, total: 17.3 ms\n",
      "Wall time: 1min 11s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "count = cached_rdd.filter(inside).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.14109072\n"
     ]
    }
   ],
   "source": [
    "pi = 4 * count / num_samples\n",
    "print(pi)\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Word Count\n",
    "\n",
    "No Spark course is complete without a word count example. However, our approach to it is DIY: It's your turn to use Spark to count the words in the given text and output the 10 most frequent words in descending order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_path = \"../.assets/data/iliad/iliad.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r--@ 1 cls  staff   1.1M May 27 21:57 ../.assets/data/iliad/iliad.txt\n",
      "The Project Gutenberg EBook of The Iliad of Homer by Homer\n",
      "\n",
      "\n",
      "\n",
      "This eBook is for the use of anyone anywhere at no cost and with almost no\n",
      "restrictions whatsoever. You may copy it, give it away or re-use it under\n",
      "the terms of the Project Gutenberg License included with this eBook or\n",
      "online at http://www.gutenberg.org/license\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!ls -lah {text_path}\n",
    "!head {text_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your turn - DIY word count in Spark\n",
    "sc = pyspark.SparkContext(appName=\"WordCount\")\n",
    "text_file = sc.textFile(text_path)\n",
    "# TODO:\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "_This notebook is licensed under a [Creative Commons Attribution 4.0 International License (CC BY 4.0)](https://creativecommons.org/licenses/by/4.0/). Copyright © 2018 [Point 8 GmbH](https://point-8.de)_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
