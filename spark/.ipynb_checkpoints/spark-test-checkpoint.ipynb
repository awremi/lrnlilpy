{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is a test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This notebook tests whether PySpark and all packages required are installed properly and working.**\n",
    "\n",
    "Please run the cell below (select and press <kbd>SHIFT+ENTER</kbd>). You should see the following output on the last line: \n",
    "\n",
    "```python\n",
    "Congratulations, your PySpark stack is ready to go\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PySpark via Jupyter Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your Python interpreter is ready. Your version: \n",
      "        3.6.5 (default, Apr 25 2018, 14:23:58) \n",
      "[GCC 4.2.1 Compatible Apple LLVM 9.1.0 (clang-902.0.39.1)]\n",
      "        \n",
      "\n",
      "Congratulations, your PySpark stack is ready to go\n"
     ]
    }
   ],
   "source": [
    "fail = False\n",
    "try:\n",
    "    import sys\n",
    "    version_string = sys.version\n",
    "    version_parts = version_string.split(\".\")\n",
    "    major = int(version_parts[0])\n",
    "    minor = int(version_parts[1])\n",
    "    if (major) >= 3 and (minor >= 6):\n",
    "        print(f\"\"\"Your Python interpreter is ready. Your version: \n",
    "        {version_string}\n",
    "        \"\"\")\n",
    "    else:\n",
    "        print(f\"\"\"Your version of Python is older than required: \n",
    "            {version_string}\n",
    "        \"\"\")\n",
    "        fail = True\n",
    "except:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    import pandas\n",
    "except ImportError:\n",
    "    print(f\"\"\"Importing package failed: pandas\"\"\")\n",
    "    fail = True\n",
    "\n",
    "try:\n",
    "    import findspark\n",
    "    findspark.init()\n",
    "except ImportError:\n",
    "    print(f\"\"\"Importing package failed: findspark\"\"\")\n",
    "    fail = True\n",
    "    \n",
    "try:\n",
    "    import pyspark\n",
    "except ImportError():\n",
    "    print(f\"\"\"Importing package failed: pyspark\"\"\")\n",
    "    fail = True\n",
    "\n",
    "if not fail:\n",
    "    print(\"\")\n",
    "    print(f\"\"\"Congratulations, your PySpark stack is ready to go\"\"\")\n",
    "else:\n",
    "    print(\"\")\n",
    "    print(\"Your Python stack is not ready, please check error messages above\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PySpark Batch Jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now evaluate the cells below. This creates a script that is then submitted to your PySpark installation. Verify the output: You should see something like this:\n",
    "\n",
    "    ##########################################\n",
    "    PySpark uses Python version:  3.6.5 (default, Apr 25 2018, 14:23:58) \n",
    "    [GCC 4.2.1 Compatible Apple LLVM 9.1.0 (clang-902.0.39.1)]\n",
    "    Congratulations, submitting a PySpark job is working\n",
    "    ##########################################\n",
    "\n",
    "\n",
    "Make sure PySpark is using the right Python version. This can be achieved by setting the environment variable `PYSPARK_PYTHON` to the appropriate Python binary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting scripts/spark_job_test.py\n"
     ]
    }
   ],
   "source": [
    "%%file scripts/spark_job_test.py\n",
    "\n",
    "SPARK_APP_NAME='sparkjob_test'\n",
    "\n",
    "import sys\n",
    "from contextlib import contextmanager\n",
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "@contextmanager\n",
    "def use_spark_context(appName):\n",
    "    conf = SparkConf().setAppName(appName) \n",
    "    spark_context = SparkContext(conf=conf)\n",
    "\n",
    "    try:\n",
    "        print(\"starting \", SPARK_APP_NAME)\n",
    "        yield spark_context\n",
    "    finally:\n",
    "        spark_context.stop()\n",
    "        print(\"stopping \", SPARK_APP_NAME)\n",
    "\n",
    "\n",
    "with use_spark_context(appName=SPARK_APP_NAME) as sc:\n",
    "    rdd = sc.range(100)\n",
    "    print()\n",
    "    print(\"##########################################\")\n",
    "    print(\"PySpark uses Python version: \", sys.version)\n",
    "    print(\"Congratulations, submitting a PySpark job is working\")\n",
    "    print(\"##########################################\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-28 22:42:47 WARN  NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "2018-05-28 22:42:48 INFO  SparkContext:54 - Running Spark version 2.3.0\n",
      "2018-05-28 22:42:48 INFO  SparkContext:54 - Submitted application: sparkjob_test\n",
      "2018-05-28 22:42:48 INFO  SecurityManager:54 - Changing view acls to: cls\n",
      "2018-05-28 22:42:48 INFO  SecurityManager:54 - Changing modify acls to: cls\n",
      "2018-05-28 22:42:48 INFO  SecurityManager:54 - Changing view acls groups to: \n",
      "2018-05-28 22:42:48 INFO  SecurityManager:54 - Changing modify acls groups to: \n",
      "2018-05-28 22:42:48 INFO  SecurityManager:54 - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(cls); groups with view permissions: Set(); users  with modify permissions: Set(cls); groups with modify permissions: Set()\n",
      "2018-05-28 22:42:48 INFO  Utils:54 - Successfully started service 'sparkDriver' on port 53583.\n",
      "2018-05-28 22:42:48 INFO  SparkEnv:54 - Registering MapOutputTracker\n",
      "2018-05-28 22:42:48 INFO  SparkEnv:54 - Registering BlockManagerMaster\n",
      "2018-05-28 22:42:48 INFO  BlockManagerMasterEndpoint:54 - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "2018-05-28 22:42:48 INFO  BlockManagerMasterEndpoint:54 - BlockManagerMasterEndpoint up\n",
      "2018-05-28 22:42:48 INFO  DiskBlockManager:54 - Created local directory at /private/var/folders/kp/ws_ljfcj7nlgg3ztmd10tyrw0000gn/T/blockmgr-350ad045-6948-482e-a754-bb6b148b7822\n",
      "2018-05-28 22:42:48 INFO  MemoryStore:54 - MemoryStore started with capacity 366.3 MB\n",
      "2018-05-28 22:42:48 INFO  SparkEnv:54 - Registering OutputCommitCoordinator\n",
      "2018-05-28 22:42:49 INFO  log:192 - Logging initialized @3360ms\n",
      "2018-05-28 22:42:49 INFO  Server:346 - jetty-9.3.z-SNAPSHOT\n",
      "2018-05-28 22:42:49 INFO  Server:414 - Started @3556ms\n",
      "2018-05-28 22:42:49 INFO  AbstractConnector:278 - Started ServerConnector@51cc4e9b{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}\n",
      "2018-05-28 22:42:49 INFO  Utils:54 - Successfully started service 'SparkUI' on port 4040.\n",
      "2018-05-28 22:42:49 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@6824aeef{/jobs,null,AVAILABLE,@Spark}\n",
      "2018-05-28 22:42:49 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@3e740a1a{/jobs/json,null,AVAILABLE,@Spark}\n",
      "2018-05-28 22:42:49 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@1a409a94{/jobs/job,null,AVAILABLE,@Spark}\n",
      "2018-05-28 22:42:49 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@1ae05324{/jobs/job/json,null,AVAILABLE,@Spark}\n",
      "2018-05-28 22:42:49 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@45aaeb8{/stages,null,AVAILABLE,@Spark}\n",
      "2018-05-28 22:42:49 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@1ad0109d{/stages/json,null,AVAILABLE,@Spark}\n",
      "2018-05-28 22:42:49 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@45136dcf{/stages/stage,null,AVAILABLE,@Spark}\n",
      "2018-05-28 22:42:49 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@164298a6{/stages/stage/json,null,AVAILABLE,@Spark}\n",
      "2018-05-28 22:42:49 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@4861e2b4{/stages/pool,null,AVAILABLE,@Spark}\n",
      "2018-05-28 22:42:49 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@6a4df2d{/stages/pool/json,null,AVAILABLE,@Spark}\n",
      "2018-05-28 22:42:49 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@6b6fdc8e{/storage,null,AVAILABLE,@Spark}\n",
      "2018-05-28 22:42:49 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@696afeed{/storage/json,null,AVAILABLE,@Spark}\n",
      "2018-05-28 22:42:49 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@2ed9d536{/storage/rdd,null,AVAILABLE,@Spark}\n",
      "2018-05-28 22:42:49 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@e243a77{/storage/rdd/json,null,AVAILABLE,@Spark}\n",
      "2018-05-28 22:42:49 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@1bba0c91{/environment,null,AVAILABLE,@Spark}\n",
      "2018-05-28 22:42:49 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@2599149d{/environment/json,null,AVAILABLE,@Spark}\n",
      "2018-05-28 22:42:49 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@4509057d{/executors,null,AVAILABLE,@Spark}\n",
      "2018-05-28 22:42:49 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@6a516829{/executors/json,null,AVAILABLE,@Spark}\n",
      "2018-05-28 22:42:49 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@21c43f76{/executors/threadDump,null,AVAILABLE,@Spark}\n",
      "2018-05-28 22:42:49 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@4c14fa2f{/executors/threadDump/json,null,AVAILABLE,@Spark}\n",
      "2018-05-28 22:42:49 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@41bb2c3d{/static,null,AVAILABLE,@Spark}\n",
      "2018-05-28 22:42:49 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@2f17aa5e{/,null,AVAILABLE,@Spark}\n",
      "2018-05-28 22:42:49 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@551931a5{/api,null,AVAILABLE,@Spark}\n",
      "2018-05-28 22:42:49 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@54231709{/jobs/job/kill,null,AVAILABLE,@Spark}\n",
      "2018-05-28 22:42:49 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@308cc480{/stages/stage/kill,null,AVAILABLE,@Spark}\n",
      "2018-05-28 22:42:49 INFO  SparkUI:54 - Bound SparkUI to 0.0.0.0, and started at http://clsmba:4040\n",
      "2018-05-28 22:42:50 INFO  SparkContext:54 - Added file file:/Users/cls/Documents/IndependentDataScience/Projects/point8/data-science-101/notebooks/spark/scripts/spark_job_test.py at file:/Users/cls/Documents/IndependentDataScience/Projects/point8/data-science-101/notebooks/spark/scripts/spark_job_test.py with timestamp 1527540170038\n",
      "2018-05-28 22:42:50 INFO  Utils:54 - Copying /Users/cls/Documents/IndependentDataScience/Projects/point8/data-science-101/notebooks/spark/scripts/spark_job_test.py to /private/var/folders/kp/ws_ljfcj7nlgg3ztmd10tyrw0000gn/T/spark-bf171fe7-487e-403c-bf9a-e6449216aada/userFiles-93c1d3ae-9ead-4e55-b645-d2236bd48c97/spark_job_test.py\n",
      "2018-05-28 22:42:50 INFO  Executor:54 - Starting executor ID driver on host localhost\n",
      "2018-05-28 22:42:50 INFO  Utils:54 - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 53584.\n",
      "2018-05-28 22:42:50 INFO  NettyBlockTransferService:54 - Server created on clsmba:53584\n",
      "2018-05-28 22:42:50 INFO  BlockManager:54 - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "2018-05-28 22:42:50 INFO  BlockManagerMaster:54 - Registering BlockManager BlockManagerId(driver, clsmba, 53584, None)\n",
      "2018-05-28 22:42:50 INFO  BlockManagerMasterEndpoint:54 - Registering block manager clsmba:53584 with 366.3 MB RAM, BlockManagerId(driver, clsmba, 53584, None)\n",
      "2018-05-28 22:42:50 INFO  BlockManagerMaster:54 - Registered BlockManager BlockManagerId(driver, clsmba, 53584, None)\n",
      "2018-05-28 22:42:50 INFO  BlockManager:54 - Initialized BlockManager: BlockManagerId(driver, clsmba, 53584, None)\n",
      "2018-05-28 22:42:50 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@2df6afcd{/metrics/json,null,AVAILABLE,@Spark}\n",
      "starting  sparkjob_test\n",
      "\n",
      "##########################################\n",
      "PySpark uses Python version:  3.6.5 (default, Apr 25 2018, 14:23:58) \n",
      "[GCC 4.2.1 Compatible Apple LLVM 9.1.0 (clang-902.0.39.1)]\n",
      "Congratulations, submitting a PySpark job is working\n",
      "##########################################\n",
      "\n",
      "2018-05-28 22:42:51 INFO  AbstractConnector:318 - Stopped Spark@51cc4e9b{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}\n",
      "2018-05-28 22:42:51 INFO  SparkUI:54 - Stopped Spark web UI at http://clsmba:4040\n",
      "2018-05-28 22:42:51 INFO  MapOutputTrackerMasterEndpoint:54 - MapOutputTrackerMasterEndpoint stopped!\n",
      "2018-05-28 22:42:51 INFO  MemoryStore:54 - MemoryStore cleared\n",
      "2018-05-28 22:42:51 INFO  BlockManager:54 - BlockManager stopped\n",
      "2018-05-28 22:42:51 INFO  BlockManagerMaster:54 - BlockManagerMaster stopped\n",
      "2018-05-28 22:42:51 INFO  OutputCommitCoordinator$OutputCommitCoordinatorEndpoint:54 - OutputCommitCoordinator stopped!\n",
      "2018-05-28 22:42:51 INFO  SparkContext:54 - Successfully stopped SparkContext\n",
      "stopping  sparkjob_test\n",
      "2018-05-28 22:42:51 INFO  ShutdownHookManager:54 - Shutdown hook called\n",
      "2018-05-28 22:42:51 INFO  ShutdownHookManager:54 - Deleting directory /private/var/folders/kp/ws_ljfcj7nlgg3ztmd10tyrw0000gn/T/spark-bf171fe7-487e-403c-bf9a-e6449216aada/pyspark-7d88268c-5320-4f54-9ef5-21f4883083b7\n",
      "2018-05-28 22:42:51 INFO  ShutdownHookManager:54 - Deleting directory /private/var/folders/kp/ws_ljfcj7nlgg3ztmd10tyrw0000gn/T/spark-bf171fe7-487e-403c-bf9a-e6449216aada\n",
      "2018-05-28 22:42:51 INFO  ShutdownHookManager:54 - Deleting directory /private/var/folders/kp/ws_ljfcj7nlgg3ztmd10tyrw0000gn/T/spark-3106a5ea-ce54-45ec-9b42-c0beb01c5817\n"
     ]
    }
   ],
   "source": [
    "!spark-submit scripts/spark_job_test.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "_This notebook is licensed under a [Creative Commons Attribution 4.0 International License (CC BY 4.0)](https://creativecommons.org/licenses/by/4.0/). Copyright Â© 2018 [Point 8 GmbH](https://point-8.de)_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
