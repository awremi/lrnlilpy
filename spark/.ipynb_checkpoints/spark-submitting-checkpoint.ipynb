{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submitting a Spark Job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Besides working with a Spark cluster interactively, for example via the PySpark console or a Jupyter notebook, a typical way of running Spark programs is to submit them as a [batch job](https://en.wikipedia.org/wiki/Batch_processing): We send a Python script to the cluster via the `spark-submit` command. This chapter explains how to do this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure that PySpark uses the right Python version for the following examples by setting the environment variable `PYSPARK_PYTHON` to Python 3. A way to do this with Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PYSPARK_PYTHON\"] = \"python3\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## spark-submit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`spark-submit` is the command line tool to send jobs to a Spark cluster. It supports Spark jobs written in Java, Scala, or Python. Additionally it offers various configuration options for tuning the performance of the job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage: spark-submit [options] <app jar | python file | R file> [app arguments]\n",
      "Usage: spark-submit --kill [submission ID] --master [spark://...]\n",
      "Usage: spark-submit --status [submission ID] --master [spark://...]\n",
      "Usage: spark-submit run-example [options] example-class [example args]\n",
      "\n",
      "Options:\n",
      "  --master MASTER_URL         spark://host:port, mesos://host:port, yarn,\n",
      "                              k8s://https://host:port, or local (Default: local[*]).\n",
      "  --deploy-mode DEPLOY_MODE   Whether to launch the driver program locally (\"client\") or\n",
      "                              on one of the worker machines inside the cluster (\"cluster\")\n",
      "                              (Default: client).\n",
      "  --class CLASS_NAME          Your application's main class (for Java / Scala apps).\n",
      "  --name NAME                 A name of your application.\n",
      "  --jars JARS                 Comma-separated list of jars to include on the driver\n",
      "                              and executor classpaths.\n",
      "  --packages                  Comma-separated list of maven coordinates of jars to include\n",
      "                              on the driver and executor classpaths. Will search the local\n",
      "                              maven repo, then maven central and any additional remote\n",
      "                              repositories given by --repositories. The format for the\n",
      "                              coordinates should be groupId:artifactId:version.\n",
      "  --exclude-packages          Comma-separated list of groupId:artifactId, to exclude while\n",
      "                              resolving the dependencies provided in --packages to avoid\n",
      "                              dependency conflicts.\n",
      "  --repositories              Comma-separated list of additional remote repositories to\n",
      "                              search for the maven coordinates given with --packages.\n",
      "  --py-files PY_FILES         Comma-separated list of .zip, .egg, or .py files to place\n",
      "                              on the PYTHONPATH for Python apps.\n",
      "  --files FILES               Comma-separated list of files to be placed in the working\n",
      "                              directory of each executor. File paths of these files\n",
      "                              in executors can be accessed via SparkFiles.get(fileName).\n",
      "\n",
      "  --conf PROP=VALUE           Arbitrary Spark configuration property.\n",
      "  --properties-file FILE      Path to a file from which to load extra properties. If not\n",
      "                              specified, this will look for conf/spark-defaults.conf.\n",
      "\n",
      "  --driver-memory MEM         Memory for driver (e.g. 1000M, 2G) (Default: 1024M).\n",
      "  --driver-java-options       Extra Java options to pass to the driver.\n",
      "  --driver-library-path       Extra library path entries to pass to the driver.\n",
      "  --driver-class-path         Extra class path entries to pass to the driver. Note that\n",
      "                              jars added with --jars are automatically included in the\n",
      "                              classpath.\n",
      "\n",
      "  --executor-memory MEM       Memory per executor (e.g. 1000M, 2G) (Default: 1G).\n",
      "\n",
      "  --proxy-user NAME           User to impersonate when submitting the application.\n",
      "                              This argument does not work with --principal / --keytab.\n",
      "\n",
      "  --help, -h                  Show this help message and exit.\n",
      "  --verbose, -v               Print additional debug output.\n",
      "  --version,                  Print the version of current Spark.\n",
      "\n",
      " Cluster deploy mode only:\n",
      "  --driver-cores NUM          Number of cores used by the driver, only in cluster mode\n",
      "                              (Default: 1).\n",
      "\n",
      " Spark standalone or Mesos with cluster deploy mode only:\n",
      "  --supervise                 If given, restarts the driver on failure.\n",
      "  --kill SUBMISSION_ID        If given, kills the driver specified.\n",
      "  --status SUBMISSION_ID      If given, requests the status of the driver specified.\n",
      "\n",
      " Spark standalone and Mesos only:\n",
      "  --total-executor-cores NUM  Total cores for all executors.\n",
      "\n",
      " Spark standalone and YARN only:\n",
      "  --executor-cores NUM        Number of cores per executor. (Default: 1 in YARN mode,\n",
      "                              or all available cores on the worker in standalone mode)\n",
      "\n",
      " YARN-only:\n",
      "  --queue QUEUE_NAME          The YARN queue to submit to (Default: \"default\").\n",
      "  --num-executors NUM         Number of executors to launch (Default: 2).\n",
      "                              If dynamic allocation is enabled, the initial number of\n",
      "                              executors will be at least NUM.\n",
      "  --archives ARCHIVES         Comma separated list of archives to be extracted into the\n",
      "                              working directory of each executor.\n",
      "  --principal PRINCIPAL       Principal to be used to login to KDC, while running on\n",
      "                              secure HDFS.\n",
      "  --keytab KEYTAB             The full path to the file that contains the keytab for the\n",
      "                              principal specified above. This keytab will be copied to\n",
      "                              the node running the Application Master via the Secure\n",
      "                              Distributed Cache, for renewing the login tickets and the\n",
      "                              delegation tokens periodically.\n",
      "      \n"
     ]
    }
   ],
   "source": [
    "!spark-submit --help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Minimal Spark Job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is a minimal example of a Python script containing a Spark job:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting scripts/spark_job_minimal.py\n"
     ]
    }
   ],
   "source": [
    "%%file scripts/spark_job_minimal.py\n",
    "\n",
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "SPARK_APP_NAME='sparkjob_template'\n",
    "conf = SparkConf().setAppName(SPARK_APP_NAME) \n",
    "spark_context = SparkContext(conf=conf)\n",
    "\n",
    "#----------------------\n",
    "# TODO: replace with your Spark code\n",
    "rdd = spark_context.range(100)\n",
    "#----------------------\n",
    "\n",
    "spark_context.stop() # don't forget to cleanly shut down\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The %%file command is an IPython \"cell magic\" that automatically writes the code from the cell to a file. So, we can directly submit this to the cluster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-22 14:39:19 WARN  NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "2018-10-22 14:39:21 INFO  SparkContext:54 - Running Spark version 2.3.0\n",
      "2018-10-22 14:39:21 INFO  SparkContext:54 - Submitted application: sparkjob_template\n",
      "2018-10-22 14:39:21 INFO  SecurityManager:54 - Changing view acls to: cls\n",
      "2018-10-22 14:39:21 INFO  SecurityManager:54 - Changing modify acls to: cls\n",
      "2018-10-22 14:39:21 INFO  SecurityManager:54 - Changing view acls groups to: \n",
      "2018-10-22 14:39:21 INFO  SecurityManager:54 - Changing modify acls groups to: \n",
      "2018-10-22 14:39:21 INFO  SecurityManager:54 - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(cls); groups with view permissions: Set(); users  with modify permissions: Set(cls); groups with modify permissions: Set()\n",
      "2018-10-22 14:39:21 INFO  Utils:54 - Successfully started service 'sparkDriver' on port 63182.\n",
      "2018-10-22 14:39:21 INFO  SparkEnv:54 - Registering MapOutputTracker\n",
      "2018-10-22 14:39:21 INFO  SparkEnv:54 - Registering BlockManagerMaster\n",
      "2018-10-22 14:39:21 INFO  BlockManagerMasterEndpoint:54 - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "2018-10-22 14:39:21 INFO  BlockManagerMasterEndpoint:54 - BlockManagerMasterEndpoint up\n",
      "2018-10-22 14:39:21 INFO  DiskBlockManager:54 - Created local directory at /private/var/folders/kp/ws_ljfcj7nlgg3ztmd10tyrw0000gn/T/blockmgr-4dd6ee31-268a-4833-8053-85698cc7c447\n",
      "2018-10-22 14:39:21 INFO  MemoryStore:54 - MemoryStore started with capacity 366.3 MB\n",
      "2018-10-22 14:39:21 INFO  SparkEnv:54 - Registering OutputCommitCoordinator\n",
      "2018-10-22 14:39:21 INFO  log:192 - Logging initialized @3536ms\n",
      "2018-10-22 14:39:21 INFO  Server:346 - jetty-9.3.z-SNAPSHOT\n",
      "2018-10-22 14:39:21 INFO  Server:414 - Started @3624ms\n",
      "2018-10-22 14:39:21 INFO  AbstractConnector:278 - Started ServerConnector@488d3cfc{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}\n",
      "2018-10-22 14:39:21 INFO  Utils:54 - Successfully started service 'SparkUI' on port 4040.\n",
      "2018-10-22 14:39:21 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@361451c5{/jobs,null,AVAILABLE,@Spark}\n",
      "2018-10-22 14:39:21 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@5d404756{/jobs/json,null,AVAILABLE,@Spark}\n",
      "2018-10-22 14:39:21 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@6f4d8732{/jobs/job,null,AVAILABLE,@Spark}\n",
      "2018-10-22 14:39:21 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@162b2b66{/jobs/job/json,null,AVAILABLE,@Spark}\n",
      "2018-10-22 14:39:21 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@498ea402{/stages,null,AVAILABLE,@Spark}\n",
      "2018-10-22 14:39:21 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@4a79de16{/stages/json,null,AVAILABLE,@Spark}\n",
      "2018-10-22 14:39:21 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@2513193a{/stages/stage,null,AVAILABLE,@Spark}\n",
      "2018-10-22 14:39:21 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@52338ca9{/stages/stage/json,null,AVAILABLE,@Spark}\n",
      "2018-10-22 14:39:21 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@6fd4851c{/stages/pool,null,AVAILABLE,@Spark}\n",
      "2018-10-22 14:39:21 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@6646cd4d{/stages/pool/json,null,AVAILABLE,@Spark}\n",
      "2018-10-22 14:39:21 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@43d52642{/storage,null,AVAILABLE,@Spark}\n",
      "2018-10-22 14:39:21 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@dcdac55{/storage/json,null,AVAILABLE,@Spark}\n",
      "2018-10-22 14:39:21 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@467a2495{/storage/rdd,null,AVAILABLE,@Spark}\n",
      "2018-10-22 14:39:21 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@1606b5b2{/storage/rdd/json,null,AVAILABLE,@Spark}\n",
      "2018-10-22 14:39:21 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@7c8b7606{/environment,null,AVAILABLE,@Spark}\n",
      "2018-10-22 14:39:21 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@1cc4c2c6{/environment/json,null,AVAILABLE,@Spark}\n",
      "2018-10-22 14:39:21 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@b8d1347{/executors,null,AVAILABLE,@Spark}\n",
      "2018-10-22 14:39:21 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@28859ca1{/executors/json,null,AVAILABLE,@Spark}\n",
      "2018-10-22 14:39:21 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@f19f4f1{/executors/threadDump,null,AVAILABLE,@Spark}\n",
      "2018-10-22 14:39:21 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@35f1d526{/executors/threadDump/json,null,AVAILABLE,@Spark}\n",
      "2018-10-22 14:39:21 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@5685eff4{/static,null,AVAILABLE,@Spark}\n",
      "2018-10-22 14:39:21 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@8058a36{/,null,AVAILABLE,@Spark}\n",
      "2018-10-22 14:39:21 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@4f5ac5e4{/api,null,AVAILABLE,@Spark}\n",
      "2018-10-22 14:39:21 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@b278ab2{/jobs/job/kill,null,AVAILABLE,@Spark}\n",
      "2018-10-22 14:39:21 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@29022aaf{/stages/stage/kill,null,AVAILABLE,@Spark}\n",
      "2018-10-22 14:39:21 INFO  SparkUI:54 - Bound SparkUI to 0.0.0.0, and started at http://192.168.202.185:4040\n",
      "2018-10-22 14:39:22 INFO  SparkContext:54 - Added file file:/Users/cls/Documents/Freelancing/Projects/point8/data-science-101/notebooks/spark/scripts/spark_job_minimal.py at file:/Users/cls/Documents/Freelancing/Projects/point8/data-science-101/notebooks/spark/scripts/spark_job_minimal.py with timestamp 1540211962316\n",
      "2018-10-22 14:39:22 INFO  Utils:54 - Copying /Users/cls/Documents/Freelancing/Projects/point8/data-science-101/notebooks/spark/scripts/spark_job_minimal.py to /private/var/folders/kp/ws_ljfcj7nlgg3ztmd10tyrw0000gn/T/spark-50a9255a-bbbd-4363-87e9-68249f8d4023/userFiles-0b514f45-840e-438c-9e13-b7d8da9551b3/spark_job_minimal.py\n",
      "2018-10-22 14:39:22 INFO  Executor:54 - Starting executor ID driver on host localhost\n",
      "2018-10-22 14:39:22 INFO  Utils:54 - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 63183.\n",
      "2018-10-22 14:39:22 INFO  NettyBlockTransferService:54 - Server created on 192.168.202.185:63183\n",
      "2018-10-22 14:39:22 INFO  BlockManager:54 - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "2018-10-22 14:39:22 INFO  BlockManagerMaster:54 - Registering BlockManager BlockManagerId(driver, 192.168.202.185, 63183, None)\n",
      "2018-10-22 14:39:22 INFO  BlockManagerMasterEndpoint:54 - Registering block manager 192.168.202.185:63183 with 366.3 MB RAM, BlockManagerId(driver, 192.168.202.185, 63183, None)\n",
      "2018-10-22 14:39:22 INFO  BlockManagerMaster:54 - Registered BlockManager BlockManagerId(driver, 192.168.202.185, 63183, None)\n",
      "2018-10-22 14:39:22 INFO  BlockManager:54 - Initialized BlockManager: BlockManagerId(driver, 192.168.202.185, 63183, None)\n",
      "2018-10-22 14:39:23 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@183474be{/metrics/json,null,AVAILABLE,@Spark}\n",
      "2018-10-22 14:39:23 INFO  AbstractConnector:318 - Stopped Spark@488d3cfc{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}\n",
      "2018-10-22 14:39:23 INFO  SparkUI:54 - Stopped Spark web UI at http://192.168.202.185:4040\n",
      "2018-10-22 14:39:23 INFO  MapOutputTrackerMasterEndpoint:54 - MapOutputTrackerMasterEndpoint stopped!\n",
      "2018-10-22 14:39:23 INFO  MemoryStore:54 - MemoryStore cleared\n",
      "2018-10-22 14:39:23 INFO  BlockManager:54 - BlockManager stopped\n",
      "2018-10-22 14:39:23 INFO  BlockManagerMaster:54 - BlockManagerMaster stopped\n",
      "2018-10-22 14:39:23 INFO  OutputCommitCoordinator$OutputCommitCoordinatorEndpoint:54 - OutputCommitCoordinator stopped!\n",
      "2018-10-22 14:39:23 INFO  SparkContext:54 - Successfully stopped SparkContext\n",
      "2018-10-22 14:39:24 INFO  ShutdownHookManager:54 - Shutdown hook called\n",
      "2018-10-22 14:39:24 INFO  ShutdownHookManager:54 - Deleting directory /private/var/folders/kp/ws_ljfcj7nlgg3ztmd10tyrw0000gn/T/spark-50a9255a-bbbd-4363-87e9-68249f8d4023/pyspark-cc3ff146-284b-4a0e-b405-52fa6782daa2\n",
      "2018-10-22 14:39:24 INFO  ShutdownHookManager:54 - Deleting directory /private/var/folders/kp/ws_ljfcj7nlgg3ztmd10tyrw0000gn/T/spark-50a9255a-bbbd-4363-87e9-68249f8d4023\n",
      "2018-10-22 14:39:24 INFO  ShutdownHookManager:54 - Deleting directory /private/var/folders/kp/ws_ljfcj7nlgg3ztmd10tyrw0000gn/T/spark-96126d13-951b-452d-95c0-2a310f9227c6\n"
     ]
    }
   ],
   "source": [
    "!spark-submit scripts/spark_job_minimal.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More Convenience"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a slightly more elaborate and convenient example that serves nicely as a template for writing Spark jobs.`contextlib` enables us to use the `with` statement to create and close a spark context in a very concise and clean way, and separate that from our actual Spark program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting scripts/spark_job_template.py\n"
     ]
    }
   ],
   "source": [
    "%%file scripts/spark_job_template.py\n",
    "\n",
    "SPARK_APP_NAME='sparkjob_template'\n",
    "\n",
    "from contextlib import contextmanager\n",
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "@contextmanager\n",
    "def use_spark_context(appName):\n",
    "    conf = SparkConf().setAppName(appName) \n",
    "    spark_context = SparkContext(conf=conf)\n",
    "\n",
    "    try:\n",
    "        print(\"starting \", appName)\n",
    "        yield spark_context\n",
    "    finally:\n",
    "        spark_context.stop()\n",
    "        print(\"stopping \", appName)\n",
    "\n",
    "\n",
    "with use_spark_context(appName=SPARK_APP_NAME) as sc:\n",
    "    #----------------------\n",
    "    # TODO: replace with your Spark code\n",
    "    rdd = sc.range(100)\n",
    "    #----------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-22 14:40:18 WARN  NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "2018-10-22 14:40:19 INFO  SparkContext:54 - Running Spark version 2.3.0\n",
      "2018-10-22 14:40:19 INFO  SparkContext:54 - Submitted application: sparkjob_template\n",
      "2018-10-22 14:40:19 INFO  SecurityManager:54 - Changing view acls to: cls\n",
      "2018-10-22 14:40:19 INFO  SecurityManager:54 - Changing modify acls to: cls\n",
      "2018-10-22 14:40:19 INFO  SecurityManager:54 - Changing view acls groups to: \n",
      "2018-10-22 14:40:19 INFO  SecurityManager:54 - Changing modify acls groups to: \n",
      "2018-10-22 14:40:19 INFO  SecurityManager:54 - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(cls); groups with view permissions: Set(); users  with modify permissions: Set(cls); groups with modify permissions: Set()\n",
      "2018-10-22 14:40:19 INFO  Utils:54 - Successfully started service 'sparkDriver' on port 63189.\n",
      "2018-10-22 14:40:19 INFO  SparkEnv:54 - Registering MapOutputTracker\n",
      "2018-10-22 14:40:19 INFO  SparkEnv:54 - Registering BlockManagerMaster\n",
      "2018-10-22 14:40:19 INFO  BlockManagerMasterEndpoint:54 - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "2018-10-22 14:40:19 INFO  BlockManagerMasterEndpoint:54 - BlockManagerMasterEndpoint up\n",
      "2018-10-22 14:40:19 INFO  DiskBlockManager:54 - Created local directory at /private/var/folders/kp/ws_ljfcj7nlgg3ztmd10tyrw0000gn/T/blockmgr-db13b117-e991-44c0-acc3-7cafde7025d6\n",
      "2018-10-22 14:40:19 INFO  MemoryStore:54 - MemoryStore started with capacity 366.3 MB\n",
      "2018-10-22 14:40:19 INFO  SparkEnv:54 - Registering OutputCommitCoordinator\n",
      "2018-10-22 14:40:19 INFO  log:192 - Logging initialized @2632ms\n",
      "2018-10-22 14:40:20 INFO  Server:346 - jetty-9.3.z-SNAPSHOT\n",
      "2018-10-22 14:40:20 INFO  Server:414 - Started @2738ms\n",
      "2018-10-22 14:40:20 INFO  AbstractConnector:278 - Started ServerConnector@6d366e9f{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}\n",
      "2018-10-22 14:40:20 INFO  Utils:54 - Successfully started service 'SparkUI' on port 4040.\n",
      "2018-10-22 14:40:20 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@faba76d{/jobs,null,AVAILABLE,@Spark}\n",
      "2018-10-22 14:40:20 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@13446247{/jobs/json,null,AVAILABLE,@Spark}\n",
      "2018-10-22 14:40:20 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@24b913a4{/jobs/job,null,AVAILABLE,@Spark}\n",
      "2018-10-22 14:40:20 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@69c40b09{/jobs/job/json,null,AVAILABLE,@Spark}\n",
      "2018-10-22 14:40:20 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@59a21a2c{/stages,null,AVAILABLE,@Spark}\n",
      "2018-10-22 14:40:20 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@356a168f{/stages/json,null,AVAILABLE,@Spark}\n",
      "2018-10-22 14:40:20 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@509df805{/stages/stage,null,AVAILABLE,@Spark}\n",
      "2018-10-22 14:40:20 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@35c98ddc{/stages/stage/json,null,AVAILABLE,@Spark}\n",
      "2018-10-22 14:40:20 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@a2b06b0{/stages/pool,null,AVAILABLE,@Spark}\n",
      "2018-10-22 14:40:20 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@52517864{/stages/pool/json,null,AVAILABLE,@Spark}\n",
      "2018-10-22 14:40:20 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@135b16b2{/storage,null,AVAILABLE,@Spark}\n",
      "2018-10-22 14:40:20 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@6a85ce68{/storage/json,null,AVAILABLE,@Spark}\n",
      "2018-10-22 14:40:20 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@38494b0e{/storage/rdd,null,AVAILABLE,@Spark}\n",
      "2018-10-22 14:40:20 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@6102863b{/storage/rdd/json,null,AVAILABLE,@Spark}\n",
      "2018-10-22 14:40:20 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@2aa7f22f{/environment,null,AVAILABLE,@Spark}\n",
      "2018-10-22 14:40:20 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@6e959f9d{/environment/json,null,AVAILABLE,@Spark}\n",
      "2018-10-22 14:40:20 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@1cf6b87a{/executors,null,AVAILABLE,@Spark}\n",
      "2018-10-22 14:40:20 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@6930d681{/executors/json,null,AVAILABLE,@Spark}\n",
      "2018-10-22 14:40:20 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@7c937702{/executors/threadDump,null,AVAILABLE,@Spark}\n",
      "2018-10-22 14:40:20 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@3e38667a{/executors/threadDump/json,null,AVAILABLE,@Spark}\n",
      "2018-10-22 14:40:20 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@17a42caf{/static,null,AVAILABLE,@Spark}\n",
      "2018-10-22 14:40:20 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@58a5e5ea{/,null,AVAILABLE,@Spark}\n",
      "2018-10-22 14:40:20 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@2cc897c9{/api,null,AVAILABLE,@Spark}\n",
      "2018-10-22 14:40:20 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@37072b14{/jobs/job/kill,null,AVAILABLE,@Spark}\n",
      "2018-10-22 14:40:20 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@6c74d8c2{/stages/stage/kill,null,AVAILABLE,@Spark}\n",
      "2018-10-22 14:40:20 INFO  SparkUI:54 - Bound SparkUI to 0.0.0.0, and started at http://192.168.202.185:4040\n",
      "2018-10-22 14:40:20 INFO  SparkContext:54 - Added file file:/Users/cls/Documents/Freelancing/Projects/point8/data-science-101/notebooks/spark/scripts/spark_job_template.py at file:/Users/cls/Documents/Freelancing/Projects/point8/data-science-101/notebooks/spark/scripts/spark_job_template.py with timestamp 1540212020673\n",
      "2018-10-22 14:40:20 INFO  Utils:54 - Copying /Users/cls/Documents/Freelancing/Projects/point8/data-science-101/notebooks/spark/scripts/spark_job_template.py to /private/var/folders/kp/ws_ljfcj7nlgg3ztmd10tyrw0000gn/T/spark-6b87c134-8bd5-4987-a8d4-372917ac3b31/userFiles-fc31debf-e7e0-4c33-96c0-a41fdf551434/spark_job_template.py\n",
      "2018-10-22 14:40:20 INFO  Executor:54 - Starting executor ID driver on host localhost\n",
      "2018-10-22 14:40:20 INFO  Utils:54 - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 63190.\n",
      "2018-10-22 14:40:20 INFO  NettyBlockTransferService:54 - Server created on 192.168.202.185:63190\n",
      "2018-10-22 14:40:20 INFO  BlockManager:54 - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "2018-10-22 14:40:20 INFO  BlockManagerMaster:54 - Registering BlockManager BlockManagerId(driver, 192.168.202.185, 63190, None)\n",
      "2018-10-22 14:40:20 INFO  BlockManagerMasterEndpoint:54 - Registering block manager 192.168.202.185:63190 with 366.3 MB RAM, BlockManagerId(driver, 192.168.202.185, 63190, None)\n",
      "2018-10-22 14:40:20 INFO  BlockManagerMaster:54 - Registered BlockManager BlockManagerId(driver, 192.168.202.185, 63190, None)\n",
      "2018-10-22 14:40:20 INFO  BlockManager:54 - Initialized BlockManager: BlockManagerId(driver, 192.168.202.185, 63190, None)\n",
      "2018-10-22 14:40:21 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@1a639504{/metrics/json,null,AVAILABLE,@Spark}\n",
      "starting  sparkjob_template\n",
      "2018-10-22 14:40:21 INFO  AbstractConnector:318 - Stopped Spark@6d366e9f{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}\n",
      "2018-10-22 14:40:21 INFO  SparkUI:54 - Stopped Spark web UI at http://192.168.202.185:4040\n",
      "2018-10-22 14:40:21 INFO  MapOutputTrackerMasterEndpoint:54 - MapOutputTrackerMasterEndpoint stopped!\n",
      "2018-10-22 14:40:22 INFO  MemoryStore:54 - MemoryStore cleared\n",
      "2018-10-22 14:40:22 INFO  BlockManager:54 - BlockManager stopped\n",
      "2018-10-22 14:40:22 INFO  BlockManagerMaster:54 - BlockManagerMaster stopped\n",
      "2018-10-22 14:40:22 INFO  OutputCommitCoordinator$OutputCommitCoordinatorEndpoint:54 - OutputCommitCoordinator stopped!\n",
      "2018-10-22 14:40:22 INFO  SparkContext:54 - Successfully stopped SparkContext\n",
      "stopping  sparkjob_template\n",
      "2018-10-22 14:40:22 INFO  ShutdownHookManager:54 - Shutdown hook called\n",
      "2018-10-22 14:40:22 INFO  ShutdownHookManager:54 - Deleting directory /private/var/folders/kp/ws_ljfcj7nlgg3ztmd10tyrw0000gn/T/spark-6b87c134-8bd5-4987-a8d4-372917ac3b31\n",
      "2018-10-22 14:40:22 INFO  ShutdownHookManager:54 - Deleting directory /private/var/folders/kp/ws_ljfcj7nlgg3ztmd10tyrw0000gn/T/spark-6b87c134-8bd5-4987-a8d4-372917ac3b31/pyspark-f414b0e4-6ab4-4539-aaca-ea6f07b4a729\n",
      "2018-10-22 14:40:22 INFO  ShutdownHookManager:54 - Deleting directory /private/var/folders/kp/ws_ljfcj7nlgg3ztmd10tyrw0000gn/T/spark-17354917-d76c-4d15-a1c2-1a9cfeb039ac\n"
     ]
    }
   ],
   "source": [
    "!spark-submit scripts/spark_job_template.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Pi Approximation as Spark Job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's your turn. Remember the $\\pi$ approximation program? Wrap this into a Spark job script and submit it to the cluster!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file scripts/pi_approximation_job.py\n",
    "\n",
    "# TODO: write a job for the pi approximation program and run it via `spark-submit`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "_This notebook is licensed under a [Creative Commons Attribution 4.0 International License (CC BY 4.0)](https://creativecommons.org/licenses/by/4.0/). Copyright © 2018 [Point 8 GmbH](https://point-8.de)_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
